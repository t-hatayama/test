{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2を正解とする\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.097500000000000031"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2の確率がもっとも高い場合\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59750000000000003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7の確率がもっとも高い場合\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082545709933802"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2の確率がもっとも高い場合\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025840929945458"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7の確率がもっとも高い場合\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2,  8, 10, 12, 12, 10,  3, 10,  1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y ,t):\n",
    "    if y.npdim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1218de240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  8.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10 , step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arrange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.sum(np.log(np.array([[0,2], [1,7], [2,0], [3,9], [4,4]]))) / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "            return np.dot(x, self.W)\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "            z = self.predict(x)\n",
    "            y = softmax(z)\n",
    "            loss = cross_entropy_error(y, t)\n",
    "            \n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48656814 -0.99194443  0.37741083]\n",
      " [-1.09829238  0.37800885  0.98644409]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.21538844 -0.06907016 -0.27429108]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8966403711666522"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = softmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8966403711666522"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 * np.log(q[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66567025  0.18425779  0.15007196]\n"
     ]
    }
   ],
   "source": [
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0692019   0.10761843 -0.17682034]\n",
      " [ 0.10380286  0.16142765 -0.26523051]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0692019   0.10761843 -0.17682034]\n",
      " [ 0.10380286  0.16142765 -0.26523051]]\n"
     ]
    }
   ],
   "source": [
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y , t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmas(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['W2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['b2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10299731,  0.0920351 ,  0.09991877,  0.09234521,  0.09443819,\n",
       "         0.1022719 ,  0.09914357,  0.10770102,  0.10862014,  0.1005288 ],\n",
       "       [ 0.10334697,  0.09169948,  0.09956568,  0.09209572,  0.09478843,\n",
       "         0.10197269,  0.09958148,  0.10790685,  0.10849311,  0.10054958],\n",
       "       [ 0.10316771,  0.09190296,  0.10014053,  0.09225853,  0.094799  ,\n",
       "         0.10196777,  0.09955026,  0.10734393,  0.10876568,  0.10010361],\n",
       "       [ 0.10358627,  0.09164736,  0.09991901,  0.09221033,  0.09460397,\n",
       "         0.10204026,  0.09978575,  0.10773158,  0.10845235,  0.1000231 ],\n",
       "       [ 0.10300713,  0.09205455,  0.10012814,  0.0925839 ,  0.09475297,\n",
       "         0.10156114,  0.09930265,  0.1075537 ,  0.10840662,  0.1006492 ],\n",
       "       [ 0.10360756,  0.09162429,  0.09993434,  0.09234354,  0.09457343,\n",
       "         0.10196063,  0.09962803,  0.10765749,  0.1084583 ,  0.10021238],\n",
       "       [ 0.10326117,  0.09189008,  0.10053983,  0.0920779 ,  0.09431105,\n",
       "         0.10205142,  0.09957722,  0.10758128,  0.10859979,  0.10011027],\n",
       "       [ 0.1036556 ,  0.09193271,  0.09953306,  0.09251547,  0.09475244,\n",
       "         0.10159602,  0.09955409,  0.10745593,  0.10861182,  0.10039287],\n",
       "       [ 0.10313024,  0.09181544,  0.09981891,  0.09248028,  0.09442258,\n",
       "         0.10222116,  0.0997098 ,  0.10784076,  0.10846219,  0.10009865],\n",
       "       [ 0.10346368,  0.09189043,  0.09974104,  0.09262503,  0.09452509,\n",
       "         0.10156671,  0.09948503,  0.10762965,  0.10835168,  0.10072165],\n",
       "       [ 0.10324915,  0.09190796,  0.10012226,  0.09267152,  0.09467969,\n",
       "         0.10165155,  0.09935426,  0.10799037,  0.10785002,  0.10052322],\n",
       "       [ 0.10327784,  0.09188094,  0.09997051,  0.09240033,  0.09452322,\n",
       "         0.10195704,  0.09935233,  0.10761443,  0.10850621,  0.10051715],\n",
       "       [ 0.10330366,  0.09201563,  0.10001099,  0.09214822,  0.09453688,\n",
       "         0.10190244,  0.09944237,  0.10791209,  0.10824553,  0.1004822 ],\n",
       "       [ 0.10331027,  0.09158066,  0.0998957 ,  0.09233087,  0.09437775,\n",
       "         0.10204396,  0.09947738,  0.10775159,  0.10884178,  0.10039004],\n",
       "       [ 0.10358278,  0.09162973,  0.09962891,  0.09218274,  0.09469451,\n",
       "         0.10187321,  0.09953993,  0.10770739,  0.10879991,  0.1003609 ],\n",
       "       [ 0.10363631,  0.0917827 ,  0.10010523,  0.09211746,  0.0946279 ,\n",
       "         0.10164769,  0.09978866,  0.10741022,  0.10850566,  0.10037817],\n",
       "       [ 0.10331013,  0.09195135,  0.100164  ,  0.09220939,  0.09463526,\n",
       "         0.10230454,  0.09942361,  0.1073898 ,  0.10829342,  0.10031852],\n",
       "       [ 0.10332797,  0.09170324,  0.10012039,  0.09211516,  0.09496359,\n",
       "         0.10172023,  0.09948564,  0.10745976,  0.10872039,  0.10038362],\n",
       "       [ 0.10311229,  0.09185248,  0.10017607,  0.09244105,  0.09468745,\n",
       "         0.10184327,  0.09983397,  0.10764264,  0.1080999 ,  0.10031088],\n",
       "       [ 0.10324311,  0.0915222 ,  0.10003504,  0.0921744 ,  0.09477702,\n",
       "         0.10180352,  0.09967027,  0.10798153,  0.10841348,  0.10037942],\n",
       "       [ 0.10314963,  0.09150988,  0.09979194,  0.09265971,  0.0942516 ,\n",
       "         0.10221381,  0.09977507,  0.10807305,  0.10856863,  0.10000666],\n",
       "       [ 0.10337309,  0.09209624,  0.09997215,  0.09221324,  0.09478375,\n",
       "         0.10178921,  0.09922983,  0.10776174,  0.10881613,  0.09996463],\n",
       "       [ 0.10294555,  0.09194365,  0.10013267,  0.09206826,  0.09481215,\n",
       "         0.10235851,  0.09939823,  0.10777466,  0.10839113,  0.1001752 ],\n",
       "       [ 0.1034279 ,  0.09181595,  0.09999784,  0.09210523,  0.0947222 ,\n",
       "         0.10198192,  0.09947994,  0.10781491,  0.10821714,  0.10043696],\n",
       "       [ 0.10347326,  0.09158534,  0.10004849,  0.09229648,  0.09456606,\n",
       "         0.1020451 ,  0.09932146,  0.10738561,  0.10869506,  0.10058316],\n",
       "       [ 0.10366449,  0.09166501,  0.09989436,  0.09222752,  0.09412566,\n",
       "         0.10177279,  0.09986087,  0.10799306,  0.10843629,  0.10035994],\n",
       "       [ 0.10349636,  0.09161064,  0.10000074,  0.09212818,  0.09448176,\n",
       "         0.10210327,  0.0996311 ,  0.10796397,  0.10853971,  0.10004427],\n",
       "       [ 0.1032967 ,  0.09208759,  0.10007762,  0.09262257,  0.09465324,\n",
       "         0.10195973,  0.09937418,  0.10721515,  0.10842711,  0.10028611],\n",
       "       [ 0.10344384,  0.09175885,  0.09992352,  0.09215607,  0.09468781,\n",
       "         0.10196571,  0.09952711,  0.1077861 ,  0.10835386,  0.10039713],\n",
       "       [ 0.10302471,  0.09161965,  0.09969261,  0.09247622,  0.09428432,\n",
       "         0.10231174,  0.09948837,  0.10807242,  0.10868106,  0.10034891],\n",
       "       [ 0.10328252,  0.09180123,  0.10013652,  0.09234554,  0.09456333,\n",
       "         0.10189838,  0.09941551,  0.10798855,  0.10821069,  0.10035773],\n",
       "       [ 0.10327895,  0.09163984,  0.10009542,  0.09235434,  0.09449799,\n",
       "         0.10205159,  0.09951546,  0.10799326,  0.10842439,  0.10014876],\n",
       "       [ 0.10324322,  0.09170873,  0.09977911,  0.09258742,  0.09482921,\n",
       "         0.10152314,  0.09958031,  0.1074064 ,  0.10905487,  0.1002876 ],\n",
       "       [ 0.10338159,  0.09186041,  0.09998143,  0.09231913,  0.09410256,\n",
       "         0.10205078,  0.09952035,  0.10776416,  0.10867514,  0.10034446],\n",
       "       [ 0.10334509,  0.09167482,  0.10001504,  0.09214059,  0.09446265,\n",
       "         0.10185046,  0.09946183,  0.10828409,  0.10863463,  0.10013079],\n",
       "       [ 0.10294877,  0.09191896,  0.09974047,  0.09232762,  0.09474168,\n",
       "         0.10232933,  0.09938096,  0.10754611,  0.10865417,  0.10041193],\n",
       "       [ 0.10338857,  0.09197386,  0.09984938,  0.0920131 ,  0.09448643,\n",
       "         0.10165911,  0.09957289,  0.10798127,  0.10857652,  0.10049888],\n",
       "       [ 0.10334843,  0.0919852 ,  0.0998272 ,  0.09227022,  0.09474651,\n",
       "         0.10189258,  0.09915433,  0.10743438,  0.10851571,  0.10082544],\n",
       "       [ 0.10337807,  0.09186561,  0.09981169,  0.09238372,  0.09494491,\n",
       "         0.10202053,  0.09938622,  0.10770707,  0.10812222,  0.10037996],\n",
       "       [ 0.10276818,  0.0919047 ,  0.1000579 ,  0.09235621,  0.09481561,\n",
       "         0.10192057,  0.09962986,  0.10765941,  0.10842314,  0.10046441],\n",
       "       [ 0.10320352,  0.09159993,  0.10010004,  0.09231288,  0.09455435,\n",
       "         0.10171448,  0.09966932,  0.10801454,  0.1081525 ,  0.10067845],\n",
       "       [ 0.10339949,  0.09191339,  0.0996326 ,  0.09242377,  0.09450548,\n",
       "         0.10208287,  0.09953876,  0.10767264,  0.10816126,  0.10066974],\n",
       "       [ 0.10320021,  0.09167224,  0.10011811,  0.09261834,  0.094616  ,\n",
       "         0.10182597,  0.09928618,  0.10757511,  0.1084582 ,  0.10062964],\n",
       "       [ 0.10344773,  0.09182442,  0.09953242,  0.09286368,  0.09452533,\n",
       "         0.10160288,  0.09955462,  0.10760564,  0.10836455,  0.10067873],\n",
       "       [ 0.10351999,  0.09174345,  0.0999367 ,  0.09222564,  0.09478984,\n",
       "         0.10169845,  0.0997498 ,  0.10753252,  0.10843486,  0.10036876],\n",
       "       [ 0.10306397,  0.09190657,  0.1002214 ,  0.09202665,  0.09479293,\n",
       "         0.10246864,  0.09943285,  0.10761402,  0.10832798,  0.10014499],\n",
       "       [ 0.10323024,  0.09137117,  0.09988973,  0.09221067,  0.09482747,\n",
       "         0.1019691 ,  0.09952394,  0.10758719,  0.10881128,  0.10057921],\n",
       "       [ 0.10316346,  0.09198172,  0.10010928,  0.09206794,  0.0947763 ,\n",
       "         0.10160299,  0.09946911,  0.107667  ,  0.10843647,  0.10072572],\n",
       "       [ 0.10337827,  0.09178409,  0.09971491,  0.09265099,  0.09458255,\n",
       "         0.10174492,  0.09935485,  0.10778072,  0.1084307 ,  0.10057802],\n",
       "       [ 0.10353111,  0.09159542,  0.10021057,  0.09207207,  0.09462744,\n",
       "         0.10191965,  0.09949786,  0.10770347,  0.10821128,  0.10063112],\n",
       "       [ 0.10355798,  0.0916818 ,  0.10013042,  0.09213006,  0.09446206,\n",
       "         0.10190706,  0.09940295,  0.10776177,  0.10837352,  0.10059238],\n",
       "       [ 0.1030517 ,  0.09169709,  0.09990322,  0.09195967,  0.09474596,\n",
       "         0.1023096 ,  0.0998413 ,  0.1076616 ,  0.10858595,  0.10024391],\n",
       "       [ 0.10356962,  0.09165652,  0.10001809,  0.09234136,  0.09483889,\n",
       "         0.1017673 ,  0.09943591,  0.10799363,  0.10813892,  0.10023975],\n",
       "       [ 0.10341146,  0.09180019,  0.10021835,  0.09256273,  0.09449817,\n",
       "         0.10170394,  0.09926914,  0.10795   ,  0.10823411,  0.1003519 ],\n",
       "       [ 0.10358192,  0.09163423,  0.10011899,  0.09239862,  0.09479737,\n",
       "         0.1015835 ,  0.09942345,  0.10751837,  0.10827908,  0.10066447],\n",
       "       [ 0.10353284,  0.09145442,  0.09955631,  0.09244304,  0.09473439,\n",
       "         0.1021093 ,  0.09941727,  0.10745513,  0.10857363,  0.10072367],\n",
       "       [ 0.10310404,  0.09198853,  0.10003652,  0.09232477,  0.09473349,\n",
       "         0.10170318,  0.09928157,  0.10781093,  0.10833372,  0.10068325],\n",
       "       [ 0.10355808,  0.09152465,  0.1001021 ,  0.09200399,  0.09489672,\n",
       "         0.10207761,  0.09953539,  0.10774821,  0.10860409,  0.09994916],\n",
       "       [ 0.10342354,  0.09167353,  0.10020289,  0.09224972,  0.09464661,\n",
       "         0.10198879,  0.09942217,  0.10754077,  0.10856459,  0.1002874 ],\n",
       "       [ 0.10324592,  0.0916776 ,  0.10003998,  0.09237359,  0.09452255,\n",
       "         0.10156837,  0.09982789,  0.10786625,  0.10855583,  0.10032202],\n",
       "       [ 0.10368882,  0.09197099,  0.09982271,  0.09232509,  0.09466551,\n",
       "         0.10158861,  0.09957845,  0.10775883,  0.10832019,  0.10028081],\n",
       "       [ 0.10344055,  0.09168201,  0.09971828,  0.09240345,  0.0943942 ,\n",
       "         0.10193521,  0.0999319 ,  0.10787252,  0.1084557 ,  0.10016618],\n",
       "       [ 0.10377571,  0.09142422,  0.10022636,  0.09210845,  0.0942597 ,\n",
       "         0.10170463,  0.09936855,  0.10808682,  0.10841432,  0.10063124],\n",
       "       [ 0.10358238,  0.09148435,  0.09991317,  0.09224057,  0.09476886,\n",
       "         0.10177413,  0.09957617,  0.10779978,  0.10842654,  0.10043406],\n",
       "       [ 0.10347956,  0.09170309,  0.10008097,  0.09227337,  0.09454547,\n",
       "         0.10203725,  0.09921239,  0.10814001,  0.10807224,  0.10045566],\n",
       "       [ 0.10333653,  0.09161774,  0.09982451,  0.09239024,  0.09418502,\n",
       "         0.10213569,  0.09964688,  0.10787864,  0.10852842,  0.10045632],\n",
       "       [ 0.10361751,  0.09185058,  0.10016613,  0.09201763,  0.0946207 ,\n",
       "         0.1018013 ,  0.09955599,  0.10754444,  0.10840558,  0.10042015],\n",
       "       [ 0.10348328,  0.09156373,  0.09981873,  0.09227898,  0.09489752,\n",
       "         0.10174188,  0.09970065,  0.10786427,  0.10802483,  0.10062612],\n",
       "       [ 0.10383913,  0.09179543,  0.10038555,  0.09241378,  0.09427803,\n",
       "         0.10170642,  0.09932782,  0.10750117,  0.10844472,  0.10030794],\n",
       "       [ 0.10349005,  0.09168927,  0.09975996,  0.09260326,  0.09463976,\n",
       "         0.1017818 ,  0.09963674,  0.10772909,  0.10840259,  0.10026747],\n",
       "       [ 0.10306789,  0.09206874,  0.10000113,  0.0920946 ,  0.09471121,\n",
       "         0.10187943,  0.0998212 ,  0.10757472,  0.1082148 ,  0.10056627],\n",
       "       [ 0.1032132 ,  0.09169699,  0.09974928,  0.0926345 ,  0.09507201,\n",
       "         0.10162013,  0.09944659,  0.10760308,  0.10844579,  0.10051841],\n",
       "       [ 0.10335977,  0.09162576,  0.099838  ,  0.09239929,  0.09441835,\n",
       "         0.10205456,  0.09953828,  0.1076671 ,  0.10848401,  0.10061488],\n",
       "       [ 0.10340042,  0.0917612 ,  0.10013188,  0.09217703,  0.09429609,\n",
       "         0.10178783,  0.09986459,  0.10758683,  0.10879224,  0.10020189],\n",
       "       [ 0.10374238,  0.09169594,  0.10008098,  0.09238738,  0.09455386,\n",
       "         0.10190437,  0.09949419,  0.10744466,  0.10835206,  0.10034417],\n",
       "       [ 0.10316344,  0.09183949,  0.10010551,  0.09222657,  0.09478539,\n",
       "         0.10172443,  0.09926978,  0.10757469,  0.10849382,  0.10081687],\n",
       "       [ 0.10338254,  0.09178487,  0.09973093,  0.09241358,  0.09446341,\n",
       "         0.10198686,  0.09967643,  0.10745587,  0.10850318,  0.10060233],\n",
       "       [ 0.10328125,  0.09186948,  0.09992408,  0.09257462,  0.09465049,\n",
       "         0.10184816,  0.09929894,  0.10779124,  0.1081815 ,  0.10058023],\n",
       "       [ 0.1035999 ,  0.09166309,  0.09970498,  0.09253766,  0.09427351,\n",
       "         0.1019893 ,  0.0995512 ,  0.10791734,  0.10877087,  0.09999216],\n",
       "       [ 0.10346403,  0.09196551,  0.10014314,  0.09219609,  0.09427764,\n",
       "         0.10210554,  0.09949586,  0.10758783,  0.10834187,  0.1004225 ],\n",
       "       [ 0.10349007,  0.0917956 ,  0.09981761,  0.09241724,  0.09451283,\n",
       "         0.10228288,  0.09946009,  0.10756157,  0.10839202,  0.10027008],\n",
       "       [ 0.10321928,  0.09174784,  0.10004325,  0.0921212 ,  0.09468971,\n",
       "         0.1019703 ,  0.09970105,  0.10765286,  0.1087178 ,  0.1001367 ],\n",
       "       [ 0.10355204,  0.09200006,  0.09966328,  0.09207686,  0.09453362,\n",
       "         0.10177893,  0.09976119,  0.10777286,  0.10829464,  0.1005665 ],\n",
       "       [ 0.10344707,  0.09151365,  0.09975172,  0.09217337,  0.09476407,\n",
       "         0.10173808,  0.09973952,  0.10774704,  0.10890088,  0.1002246 ],\n",
       "       [ 0.1033335 ,  0.0917551 ,  0.10015353,  0.09234401,  0.09476185,\n",
       "         0.1018637 ,  0.09958397,  0.10748182,  0.10828537,  0.10043715],\n",
       "       [ 0.10321913,  0.09167169,  0.10029653,  0.09248784,  0.09461506,\n",
       "         0.10225326,  0.09947486,  0.10755776,  0.10815531,  0.10026855],\n",
       "       [ 0.10333866,  0.09177491,  0.10011511,  0.09241283,  0.09436947,\n",
       "         0.101953  ,  0.09945011,  0.10766342,  0.10855371,  0.10036877],\n",
       "       [ 0.10351925,  0.0917247 ,  0.1001214 ,  0.09237259,  0.09450921,\n",
       "         0.10194495,  0.0999322 ,  0.10716922,  0.10840113,  0.10030534],\n",
       "       [ 0.10353338,  0.09202068,  0.09992685,  0.09237253,  0.09452441,\n",
       "         0.10182442,  0.09934986,  0.10772889,  0.10817814,  0.10054085],\n",
       "       [ 0.10307873,  0.09194727,  0.10005974,  0.0922487 ,  0.09470349,\n",
       "         0.10173924,  0.09984675,  0.10750215,  0.10868591,  0.10018803],\n",
       "       [ 0.10294417,  0.0920098 ,  0.10010279,  0.0921225 ,  0.09466695,\n",
       "         0.10200888,  0.09953923,  0.10757151,  0.10857352,  0.10046065],\n",
       "       [ 0.1031526 ,  0.09190083,  0.10010121,  0.09247494,  0.09458556,\n",
       "         0.10184193,  0.09942389,  0.10762743,  0.108437  ,  0.10045462],\n",
       "       [ 0.10369455,  0.09165991,  0.1003607 ,  0.09240256,  0.0944116 ,\n",
       "         0.10196085,  0.09940715,  0.10737072,  0.10827385,  0.10045812],\n",
       "       [ 0.10358851,  0.09175185,  0.10023033,  0.092457  ,  0.09455772,\n",
       "         0.10166629,  0.09956409,  0.10762559,  0.10776181,  0.10079682],\n",
       "       [ 0.10328653,  0.09188405,  0.09990873,  0.09255381,  0.09449736,\n",
       "         0.10205625,  0.09963434,  0.10755999,  0.10842989,  0.10018905],\n",
       "       [ 0.10325196,  0.09164696,  0.10005842,  0.09257295,  0.0944114 ,\n",
       "         0.10221306,  0.09952259,  0.10751602,  0.10839548,  0.10041119],\n",
       "       [ 0.10350448,  0.09148753,  0.1002189 ,  0.09244432,  0.09464348,\n",
       "         0.10193953,  0.09946812,  0.10760738,  0.10828321,  0.10040304],\n",
       "       [ 0.10332276,  0.091696  ,  0.09977096,  0.09255191,  0.09481514,\n",
       "         0.10205986,  0.09929375,  0.10783268,  0.10811835,  0.1005386 ],\n",
       "       [ 0.10371226,  0.09182458,  0.09962374,  0.09218145,  0.09459568,\n",
       "         0.10122993,  0.09984569,  0.1078873 ,  0.10840607,  0.10069329],\n",
       "       [ 0.10346998,  0.09151457,  0.09983203,  0.09221119,  0.09503886,\n",
       "         0.10161299,  0.09970287,  0.10766144,  0.10859592,  0.10036015]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(100,784)\n",
    "net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100,784)\n",
    "t = np.random.rand(100,10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n"
     ]
    }
   ],
   "source": [
    "print(grads['W1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'two_layer_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-ca1ec4cbcd13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwo_layer_net\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'two_layer_net'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "    \n",
    "train_loss_list=[]\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) #高速版！\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
